{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPIw0LuFIbtTwEeKXq/JBxC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-zjorQebcCOm"},"outputs":[],"source":["# %% Import packages\n","import datetime\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","source":["# %% Define parameters\n","time_delta = datetime.timedelta(days=14)\n","start_date = datetime.date(year=2022, month=10, day=15)"],"metadata":{"id":"i7A8Nci5cDZ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %% Define functions\n","\n","def format_date(date_string):\n","    months_encoded = {\"Jan\": \"01\", \"Feb\": \"02\", \"Mar\": \"03\", \"Apr\": \"04\",\n","                      \"May\": \"05\", \"Jun\": \"06\", \"Jul\": \"07\", \"Aug\": \"08\",\n","                      \"Sept\": \"09\", \"Oct\": \"10\", \"Nov\": \"11\", \"Dec\": \"12\"}\n","\n","    date_list = date_string.split(\"\\n\")\n","    year = int(date_list[1])\n","    month = int(months_encoded[date_list[0].split(\" \")[1]])\n","    day = int(date_list[0].split(\" \")[0])\n","    return str(datetime.date(year=year, month=month, day=day))\n","\n","def create_date(date_string):\n","    date_list = date_string.split(\"-\")\n","    year = int(date_list[0])\n","    month = int(date_list[1])\n","    day = int(date_list[2])\n","    return (datetime.date(year=year, month=month, day=day))\n","\n","def calculate_lag(trade, publish):\n","    trade_date = create_date(trade)\n","    publish_date = create_date(publish)\n","    lag = publish_date - trade_date\n","    return lag.days\n","\n","def sum_quantity(quant_list):\n","    sum_value = 0\n","    for q in quant_list:\n","        sum_value += int(q)\n","    return sum_value\n","\n","def find_unique_titles(title_object):\n","    unique_titles = []\n","    for t in title_object:\n","        if t not in unique_titles:\n","            unique_titles.append(t)\n","    return \";\".join(unique_titles)"],"metadata":{"id":"iQgBJkXDcFmF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %% Load data\n","\n","imported_all = pd.read_csv(\"insider_trades.with_titles.csv\")\n","imported_data = imported_all.dropna()\n","\n","processed_data = {}\n","processed_data[\"Insider\"] = []\n","processed_data[\"Company\"] = []\n","processed_data[\"Title\"] = []\n","processed_data[\"Traded\"] = []\n","processed_data[\"Published\"] = []\n","processed_data[\"Ticker\"] = []\n","processed_data[\"last_price\"] = []\n","processed_data[\"Qty\"] = []\n","processed_data[\"shares_held\"] = []\n","processed_data[\"Owned\"] = []\n","processed_data[\"Value\"] = []\n","\n","\n","for row in imported_data.iterrows():\n","    # Append the politician data\n","    processed_data[\"Insider\"].append(row[1][\"owner_name\"])\n","    processed_data[\"Company\"].append(row[1][\"company_name\"])\n","    processed_data[\"Title\"].append(row[1][\"Canonical_Role\"])\n","\n","    # Append the date data\n","    processed_data[\"Traded\"].append(row[1][\"trade_date\"])\n","    processed_data[\"Published\"].append(row[1][\"filing_date\"].split(\" \")[0])\n","\n","    # Apend the trade information\n","    processed_data[\"Ticker\"].append(row[1][\"ticker\"])\n","    processed_data[\"last_price\"].append(row[1][\"last_price\"].replace(\"$\", \"\"))\n","    processed_data[\"Qty\"].append(row[1][\"Qty\"].replace(\"+\", \"\").replace(\",\", \"\"))\n","    processed_data[\"shares_held\"].append(row[1][\"shares_held\"].replace(\",\", \"\"))\n","    processed_data[\"Owned\"].append(row[1][\"Owned\"].replace(\"+\", \"\").replace(\"%\", \"\").replace(\",\", \"\"))\n","    processed_data[\"Value\"].append(row[1][\"Value\"].replace(\"+\", \"\").replace(\"$\", \"\").replace(\",\", \"\"))\n","\n","processed_data_DF = pd.DataFrame(processed_data)"],"metadata":{"id":"DafNs4mrcHj3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %% Save when sorting by trade date\n","\n","processed_data_DF_sorted = processed_data_DF.sort_values(by=\"Traded\")\n","processed_data_DF_sorted.to_csv(\"insiders_by_trade_date.csv\", index=False)\n","\n","start_temp = start_date\n","end_temp = start_date + time_delta\n","end_date = create_date(max([max(processed_data_DF_sorted[\"Traded\"]), max(processed_data_DF_sorted[\"Published\"])]))\n","\n","biweekly_data = {}\n","biweekly_data[\"start_date\"] = []\n","biweekly_data[\"end_date\"] = []\n","biweekly_data[\"ticker\"] = []\n","biweekly_data[\"lag\"] = []\n","biweekly_data[\"n_trades\"] = []\n","biweekly_data[\"Qty\"] = []\n","biweekly_data[\"Title\"] = []\n","\n","while end_temp < end_date:\n","\n","    temp1 = processed_data_DF_sorted[processed_data_DF_sorted[\"Traded\"] > str(start_temp)]\n","    temp2 = temp1[temp1[\"Traded\"] < str(end_temp)]\n","\n","    for tick in np.unique(temp2[\"Ticker\"]):\n","        ticker_temp = temp2[temp2[\"Ticker\"] == tick]\n","\n","        biweekly_data[\"start_date\"].append(str(start_temp))\n","        biweekly_data[\"end_date\"].append(str(end_temp))\n","        biweekly_data[\"ticker\"].append(tick)\n","\n","        all_lags = []\n","        for entry in ticker_temp.iterrows():\n","            all_lags.append(calculate_lag(entry[1][\"Traded\"], entry[1][\"Published\"]))\n","        biweekly_data[\"lag\"].append(sum(all_lags) / len(all_lags))\n","        biweekly_data[\"n_trades\"].append(len(ticker_temp))\n","        biweekly_data[\"Qty\"].append(sum_quantity(ticker_temp[\"Qty\"]))\n","        biweekly_data[\"Title\"].append(find_unique_titles(ticker_temp[\"Title\"]))\n","\n","    start_temp = end_temp\n","    end_temp = start_temp + time_delta\n","\n","\n","pd.DataFrame(biweekly_data).to_csv(\"insiders_by_trade_date.biweekly.csv\", index=False)"],"metadata":{"id":"yS8UGnRLcJ8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %% Save when sorting by declaration date\n","\n","processed_data_DF_sorted = processed_data_DF.sort_values(by=\"Published\")\n","processed_data_DF_sorted.to_csv(\"insiders_by_declared_date.csv\", index=False)\n","\n","start_temp = start_date\n","end_temp = start_date + time_delta\n","end_date = create_date(max([max(processed_data_DF_sorted[\"Traded\"]), max(processed_data_DF_sorted[\"Published\"])]))\n","\n","biweekly_data = {}\n","biweekly_data[\"start_date\"] = []\n","biweekly_data[\"end_date\"] = []\n","biweekly_data[\"ticker\"] = []\n","biweekly_data[\"lag\"] = []\n","biweekly_data[\"n_trades\"] = []\n","biweekly_data[\"Qty\"] = []\n","biweekly_data[\"Title\"] = []\n","\n","while end_temp < end_date:\n","\n","    temp1 = processed_data_DF_sorted[processed_data_DF_sorted[\"Published\"] > str(start_temp)]\n","    temp2 = temp1[temp1[\"Published\"] < str(end_temp)]\n","\n","    for tick in np.unique(temp2[\"Ticker\"]):\n","        ticker_temp = temp2[temp2[\"Ticker\"] == tick]\n","\n","        biweekly_data[\"start_date\"].append(str(start_temp))\n","        biweekly_data[\"end_date\"].append(str(end_temp))\n","        biweekly_data[\"ticker\"].append(tick)\n","\n","        all_lags = []\n","        for entry in ticker_temp.iterrows():\n","            all_lags.append(calculate_lag(entry[1][\"Traded\"], entry[1][\"Published\"]))\n","        biweekly_data[\"lag\"].append(sum(all_lags) / len(all_lags))\n","        biweekly_data[\"n_trades\"].append(len(ticker_temp))\n","        biweekly_data[\"Qty\"].append(sum_quantity(ticker_temp[\"Qty\"]))\n","        biweekly_data[\"Title\"].append(find_unique_titles(ticker_temp[\"Title\"]))\n","\n","    start_temp = end_temp\n","    end_temp = start_temp + time_delta\n","\n","\n","pd.DataFrame(biweekly_data).to_csv(\"insiders_by_declared_date.biweekly.csv\", index=False)"],"metadata":{"id":"zuUbVIpGcMlN"},"execution_count":null,"outputs":[]}]}